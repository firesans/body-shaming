{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/danish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/danish/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/data.txt\", delimiter=',',encoding = \"utf-8\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [entry.lower() for entry in df['text']]\n",
    "df['text']= [word_tokenize(entry) for entry in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cozy', 'glam', 'aaliyah', 'inspire', 'ootd',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['guy', 'chubbygirlsdoitbetter', 'beyourself',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['beautiful', 'disaster']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['part', 'mom']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['depressed', 'piece', 'shit']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ['cozy', 'glam', 'aaliyah', 'inspire', 'ootd',...      0\n",
       "1  ['guy', 'chubbygirlsdoitbetter', 'beyourself',...      1\n",
       "2                          ['beautiful', 'disaster']      0\n",
       "3                                    ['part', 'mom']      0\n",
       "4                     ['depressed', 'piece', 'shit']      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus  = df\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index,'text'] = str(Final_words)\n",
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text'],Corpus['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='word',max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_word = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_word = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='word',ngram_range=(2,3),max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_ng = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_ng = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='char',ngram_range=(2,3),max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_chr = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_chr = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM word,n-gram,char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  91.08333333333334\n",
      "F1 Score ->  0.8595762132604239\n",
      "Precision ->  0.8737120895495267\n",
      "Recall ->  0.8473958927191416\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  88.08333333333334\n",
      "F1 Score ->  0.7948033591981547\n",
      "Precision ->  0.8491799421926464\n",
      "Recall ->  0.763248577907846\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  90.91666666666667\n",
      "F1 Score ->  0.8621124416112604\n",
      "Precision ->  0.861578947368421\n",
      "Recall ->  0.8626493355123965\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest word,char,n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  79.83333333333333\n",
      "F1 Score ->  0.4745184895663692\n",
      "Precision ->  0.8432689616568709\n",
      "Recall ->  0.5155384946726971\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  82.58333333333333\n",
      "F1 Score ->  0.5984572304115814\n",
      "Precision ->  0.8693576388888888\n",
      "Recall ->  0.5862503642329571\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  79.25\n",
      "F1 Score ->  0.4421199442119944\n",
      "Precision ->  0.39625\n",
      "Recall ->  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/Anacond/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/danish/Anacond/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  89.5\n",
      "F1 Score ->  0.8181818181818182\n",
      "Precision ->  0.8802618791877653\n",
      "Recall ->  0.7825624263615978\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  89.66666666666666\n",
      "F1 Score ->  0.8359607540513724\n",
      "Precision ->  0.8525875974653239\n",
      "Recall ->  0.8221529651729949\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  85.08333333333333\n",
      "F1 Score ->  0.6970273987094044\n",
      "Precision ->  0.8571105072463768\n",
      "Recall ->  0.6598317560462671\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow>=1.7.0 in /home/danish/Anacond/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (2.0.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.25.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (0.33.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.16.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (3.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (2.0.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (0.1.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorflow>=1.7.0) (3.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (41.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (0.15.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/danish/Anacond/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (3.1.1)\n",
      "Requirement already satisfied: h5py in /home/danish/Anacond/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow>=1.7.0) (2.9.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/danish/Anacond/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/danish/Anacond/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/danish/Anacond/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (4.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/danish/Anacond/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/danish/Anacond/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (0.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/danish/Anacond/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/danish/Anacond/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/Anacond/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/danish/Anacond/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=1.7.0) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"tensorflow>=1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "variable_scope module_2/ was unused but the corresponding name_scope was already taken.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e3eefb88291b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anacond/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such graph variant: tags=%r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mabs_state_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_get_state_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark_name_scope_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs_state_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacond/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m_try_get_state_scope\u001b[0;34m(name, mark_name_scope_used)\u001b[0m\n\u001b[1;32m    393\u001b[0m       raise RuntimeError(\n\u001b[1;32m    394\u001b[0m           \u001b[0;34m\"variable_scope %s was unused but the corresponding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m           \"name_scope was already taken.\" % abs_state_scope)\n\u001b[0m\u001b[1;32m    396\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mabs_state_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: variable_scope module_2/ was unused but the corresponding name_scope was already taken."
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AutoTrackable' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bce4149ba3fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Nothing suits me like suit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Extract ELMo features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AutoTrackable' object is not callable"
     ]
    }
   ],
   "source": [
    "x = [\"Nothing suits me like suit\"] # Extract ELMo features \n",
    "embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"] \n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
